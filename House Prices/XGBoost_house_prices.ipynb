{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Amazon SageMaker XGBoost algorithm\n",
    "## house prices prediction\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Data Preprocessing](#Data-Preprocessing)\n",
    "2. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "3. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "5. [Using SageMaker Endpoint](#Using-SageMaker-Endpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from mxnet import nd\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket='sagemaker-eu-west-1-483308273948' # put your s3 bucket name here, and create s3 bucket\n",
    "prefix = 'house_prices'\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We'll read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., a\n",
    "ssuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "label = pd.read_csv('data/label.csv', header=None)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into validation and training and breaking dataset into data and label\n",
    "\n",
    "# 80%-20% training to validation\n",
    "train = train.as_matrix()\n",
    "label = label.as_matrix()\n",
    "train_size = int(train.shape[0]*0.8)\n",
    "\n",
    "train_data  = train[:train_size,:]\n",
    "val_data = train[train_size:,:]\n",
    "\n",
    "train_label = label[:train_size]\n",
    "val_label = label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "train_data_url = \"\"\n",
    "validation_data_url = \"\"\n",
    "def to_libsvm(f, labels, values):\n",
    "     f.write(bytes('\\n'.join(\n",
    "         ['{} {}'.format(label, ' '.join(['{}:{}'.format(i + 1, el) for i, el in enumerate(vec)])) for label, vec in\n",
    "          zip(labels, values)]), 'utf-8'))\n",
    "     return f\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "partitions = [('train', (train_data,train_label)), ('validation', (val_data,val_label))]\n",
    "for partition_name, partition in partitions:\n",
    "    print('{}: {} {}'.format(partition_name, partition[0].shape, partition[1].shape))\n",
    "    labels = partition[1].tolist()\n",
    "    vectors = partition[0].tolist()\n",
    "    f = io.BytesIO()\n",
    "    to_libsvm(f, labels, vectors)\n",
    "    f.seek(0)\n",
    "    key = \"{}/csv/{}\".format(prefix,partition_name)\n",
    "    url = 's3://{}/{}'.format(bucket, key)\n",
    "    print('Writing to {}'.format(url))\n",
    "    write_to_s3(f, bucket, key)\n",
    "    print('Done writing to {}'.format(url))\n",
    "    if (partition_name == \"train\"):\n",
    "        train_data_url = url\n",
    "    else:\n",
    "        validation_data_url = url\n",
    "\n",
    "output = 's3://{}/{}'.format(bucket, prefix+'/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\n",
    "container = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "train = sagemaker.s3_input(s3_data=train_data_url,content_type='libsvm')\n",
    "validation = sagemaker.s3_input(s3_data=validation_data_url,content_type='libsvm')\n",
    "\n",
    "# Creating a new sagemaker job\n",
    "estimator = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output,\n",
    "                                       sagemaker_session=sess)\n",
    "\n",
    "# Settings the job hyperparamaters\n",
    "estimator.set_hyperparameters(eval_metric='rmse',\n",
    "                           objective=\"reg:linear\",\n",
    "                           num_round=100)\n",
    "\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1), # The eta parameter shrinks the feature weights to make the boosting process more conservative.\n",
    "                         'alpha' : ContinuousParameter(0, 2), # L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "                         'min_child_weight' : ContinuousParameter(1, 10), # Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning.\n",
    "                         'max_depth' : IntegerParameter(1, 10)} # Maximum depth of a tree.\n",
    "\n",
    "objective_metric_name = 'validation:rmse'\n",
    "\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            objective_type='Minimize',\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3)\n",
    "tuner.fit({'train': train,'validation' : validation})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "_Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "xgboost_predictor.content_type = 'text/csv'\n",
    "xgboost_predictor.serializer = json_serializer\n",
    "xgboost_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "item = 44\n",
    "print(\"Data: \\n\" + str(val_data[item].tolist()) + \"\\n\")\n",
    "print(\"Predicted price: $\" + str(round(exp(xgboost_predictor.predict(val_data[item].tolist())))))\n",
    "print(\"Real price: $\" + str(round(exp(val_label[item]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = []\n",
    "for item in test.values:\n",
    "    data = {}\n",
    "    for column in test.keys():\n",
    "        data[column] = item[test.columns.get_loc(column)]\n",
    "    data['Price'] = exp(xgboost_predictor.predict(item.tolist()))\n",
    "    prices.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.plot.scatter(x='LotArea', y='Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.Session().delete_endpoint(xgboost_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}\n",
    "for i,item in enumerate(val_data):\n",
    "    values[i] = abs(xgboost_predictor.predict(val_data[i].tolist()) - val_label[i])\n",
    "    \n",
    "import operator\n",
    "sorted_values = sorted(values.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print(sorted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-eu-west-1-483308273948/house_prices/output/xgboost-180614-1835-004-9859f1c0/output/model.tar.gz ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "model = pkl.load(open('xgboost-model','rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xgb.DMatrix(val_data[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor.predict(val_data[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
