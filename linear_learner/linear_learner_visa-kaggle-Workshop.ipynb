{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker builtin algorithm to predict fraud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocessing data\n",
    "If you are intersted in learning about preprocessing data, you should start here, otherwise you could simply start from part 2, when we load the data from npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import re\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your name to the name variable, don't change any other variable in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"\" # Can include hyphens (-), but not spaces\n",
    "region = boto3.session.Session().region_name\n",
    "bucket = 'sagemaker-eu-west-1-483308273948'\n",
    "original_key = 'visa-kaggle/original.csv'\n",
    "protocol=\"s3://\"\n",
    "datafile = 'data/original.csv'\n",
    "prefix = name+ '/dataset'\n",
    " \n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "1. Downloding file locally\n",
    "2. Loading file into pandas for inspection\n",
    "3. conversting data to numpy\n",
    "4. shuffling the data\n",
    "5. spliting data into test and training\n",
    "6. breaking up each data set to data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the file to a local folder\n",
    "!mkdir -p data\n",
    "client = boto3.client('s3')\n",
    "with open(datafile, 'wb') as f:\n",
    "    client.download_fileobj(bucket, original_key, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and understanding it's format\n",
    "Read the local CSV and understand it's format.\n",
    "Use pandas to load the file\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(datafile)\n",
    "```\n",
    "\n",
    "and read the first 5 lines using:\n",
    "\n",
    "```python\n",
    "df.head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into pandas for inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas, we can get more info about the data.\n",
    "```python \n",
    "df['Class'].count()\n",
    "``` \n",
    "shows us that there are 284,807 records in this dataset\n",
    "```python \n",
    "df[df['Class'] == 1]['Class'].count()\n",
    "``` \n",
    "but only 492 of them are fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the command above to examine the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting pandas to numpy\n",
    "We'll convert the data to numpy for data shuffling, data manipulation and extracting the labels from the dataset\n",
    "\n",
    "Run \n",
    "```python\n",
    "raw_data = df.as_matrix()\n",
    "``` \n",
    "to convert the df to numpy.  \n",
    "```python \n",
    "raw_data.shape\n",
    "``` \n",
    "will print the structure of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Data Into Numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the data and spliting between data and labels\n",
    "using \n",
    "```python\n",
    "np.random.seed(123)\n",
    "``` \n",
    "we can configure the numpy random generator to use a constant seed\n",
    "\n",
    "shuffle the data \n",
    "```\n",
    "np.random.shuffle(raw_data)\n",
    "```\n",
    "\n",
    "and split the data \n",
    "\n",
    "```\n",
    "label = raw_data[:, -1]\n",
    "data = raw_data[:, :-1]\n",
    "```\n",
    "\n",
    "let's make sure that both have the same number of records:\n",
    "```python\n",
    "print(\"label_shape = {}; data_shape= {}\".format(label.shape, data.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data and splitting between data and label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into training and validation datasets\n",
    "In this example we'll use 60% of the data for training and 40% for validation.\n",
    "\n",
    "we can get the training dataset size using:\n",
    "```python\n",
    "train_size = int(data.shape[0]*0.6)\n",
    "```\n",
    "\n",
    "we'll split both the training and validation data sets (data and labels)\n",
    "```python\n",
    "train_data  = data[:train_size, :]\n",
    "val_data = data[train_size:, :]\n",
    "\n",
    "train_label = label[:train_size]\n",
    "val_label = label[train_size:]\n",
    "```\n",
    "\n",
    "We'll verifiy the shapes:\n",
    "```python\n",
    "print(\"training data shape= {}; training label shape = {} \\nValidation data shape= {}; validation label shape = {}\".format(train_data.shape, train_label.shape,val_data.shape,val_label.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into validation and training and breaking dataset into data and label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training\n",
    "In this part we load the data from pre-processed files and train the model.\n",
    "We'll start by creating the train and test sets:\n",
    "```python\n",
    "train_set = (train_data, train_label)\n",
    "test_set = (val_data, val_label)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion\n",
    "Amazon Algorithms support csv and recordio/protobuf. recordio is faster than CSV and specially in algorithms that deal with sparse matrices.\n",
    "\n",
    "```python\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.array([t.tolist() for t in train_set[1]]).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload training data\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it.\n",
    "\n",
    "Upload to S3:\n",
    "```python\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(sagemaker_session.default_bucket()).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(sagemaker_session.default_bucket(), prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm.\n",
    "```python\n",
    "output_location = 's3://{}/{}/output'.format(sagemaker_session.default_bucket(), prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "At this point we are using an linear learner from amazon algorithms. Docker file containing the model is located in multiple regions. We tool the following steps\n",
    "1. define containers\n",
    "2. Create am Estimator object and pass the hyper-parameters as well as the model location to it.\n",
    "3. run Estimator.fit to begin training the model\n",
    "\n",
    "SageMaker uses one of thse prebuilt containers for the linear-learner built in algo\n",
    "```python\n",
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the containers dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Estimator function to create a new sagemaker trainning job\n",
    "```python\n",
    "sess = sagemaker.Session()\n",
    "linear = sagemaker.estimator.Estimator(containers[region],\n",
    "                                       role, #S3 role, so the notebook can read the data and upload the model\n",
    "                                       train_instance_count=1, #number of instances for training\n",
    "                                       train_instance_type='ml.c4.xlarge', # type of training instance\n",
    "                                       output_path=output_location, #S3 location for the trained model\n",
    "                                       sagemaker_session=sess,\n",
    "                                       base_job_name='linear-learner-' + name)\n",
    "```\n",
    "Set the hyperparamaeters\n",
    "```python\n",
    "linear.set_hyperparameters(feature_dim=30, # dataset has 30 columns (features)\n",
    "                           predictor_type='binary_classifier',\n",
    "                           mini_batch_size=200)\n",
    "```\n",
    "send the link to S3 to the trainning job and start the trainning\n",
    "```python\n",
    "linear.fit({'train': s3_train_data})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the trainning job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting the model\n",
    "We use sagemaker to host the live model by calling deploy from estimator we defined previously. This action will create a dockerized environment using ECS and permits autoscaling. \n",
    "\n",
    "```python\n",
    "linear_predictor = linear.deploy(initial_instance_count=1, #Initial number of instances. \n",
    "                                                           #Autoscaling can increase the number of instances.\n",
    "                                 instance_type='ml.m4.xlarge',# instance type\n",
    "                                 name='linear-learner-' + name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host a SageMaker endpoint for predicition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "deploy resturn a live endpoint (linear_predictor). Predictors in sagemaker accept csv and json. In this case we use json serialization.\n",
    "\n",
    "configure the predicition endpoint:\n",
    "```python\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict a single item:\n",
    "```python\n",
    "print(train_set[0][48:49])\n",
    "print(\"The data Actual label: \" + str(train_set[1][48:49][0]))\n",
    "linear_predictor.predict(train_set[0][48:49])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a confusion matrix from the validation data:\n",
    "```python\n",
    "non_zero = np.count_nonzero(test_set[1])\n",
    "zero = len(test_set[1]) - non_zero\n",
    "print(\"validation set includes: {} non zero and {} items woth value zero\".format(non_zero, zero))\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(test_set[1], predictions, rownames=['actuals'], colnames=['predictions'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix above indicates that:\n",
    "- of 162 fraudulant cases we detected 143 correcly\n",
    "- 39 times a non-fraudulant transaction has been flagges as fraud from a total of 113761 transactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "if you're ready to be done with this notebook, please run the delete_endpoint line in the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
